@ Resultados y conclusiones @

Se realizaron diferentes pruebas para definir los parámetros, estructuras y funciones
que minimizan el ECM. Luego de diferentes evaluaciones hemos fijado aquellos
parámetros que minimizaban el ECM de la población general de patrones.

En las pruebas, 100 de los 1000 parámetros fueron utilizados como conjunto de
entrenamiento. La función de rollback se encuentra activada.

En las primeras pruebas se utilizó una estructura de [2 4 4 1] neuronas en las capas, el
learning rate fue definido como 0.4 y la función de transferencia usada ha sido la
función sigmoidea lógica.

@@ Parámetros adaptativos @@

En la sección anexo se presenta una tabla que muestra los ECM para distintas configuraciones
de los parametros adaptativos en donde las columnas representan:

* a: Cantidad de pasos que se consideran una buena racha.
* b: Valor que se le suma a eta en una buena racha.
* c: Factor por el cual se disminuye eta en un paso considerado malo.
* d: Valor según el cual se evalúa si el paso es bueno, malo o neutral.

De todas las corridas realizadas, hemos decidido quedarnos con las nueve mejores,
de estas nueve escogidas hemos hecho tres corridas por cada una de ellas.
Posteriormente, hemos realizado un promedio de estas tres corridas por cada una
de las nueve mejores.
La tabla con los promedios de cinco mejores pruebas realizadas se presenta, como ya hemos mencionado, en la sección anexo, en la **Tabla 1**.

Finalmente nos hemos quedado con la primera prueba que es aquella
con la que obtuvimos mejores resultados con los siguientes valores:
a = 3, b = 0.001, c = 0.001, d = 0.01.

@@ Funciones de transferencia @@

Se evaluó la red con la función sigmoidea lógica y la tangente hiperbólica
Se realizaron tres corridas para cada una de las funciones, la tabla de
resultados se presenta en la sección anexo en la **Tabla 2**.

A partir de este momento, continuamos utilizando la función sigmoidea lógica.

@@ Cantidad de entradas @@

Hemos decidido realizar corridas para dos y tres entradas como nos fue recomendado
por la cátedra. Por cada una de estas opciones, hemos realizado tres corridas.
Exponemos los resultados en la **Tabla 3** de la sección anexo.
El mejor resultado fue obtenido para dos entradas.

@@ Learning rate @@

Otro parámetro a ajustar en las corridas realizadas era el learning rate.
Hemos decidido realizar tres corridas por cada valor escogido.
Los resultados de dichas corridas se presentan en la **Tabla 4** de la sección anexo.

El valor elegido para el learning rate fue el de 0.4 ya que a partir de él
hemos obtenido el menor ECM.

@@ Momentum @@

El siguiente parámetro a ajustar en las corridas realizadas ha sido el momentum.
Hemos decidido realizar tres corridas por cada valor escogido.
Los resultados de dichas corridas se presentan en la **Tabla 5** de la sección anexo.

El valor elegido para el momentum fue el de 1 ya que a partir de él
hemos obtenido el menor ECM.

@@ Arquitecturas @@

Hemos probado con varias arquitecturas que se presentan en la **Tabla 6** de la sección anexo.
Se han hecho tres corridas por cada arquitectura.
Hemos concluído en que la arquitectura con la cual hemos obtenido los mejores resultados
ha sido [2 9 9 1].

Finalmente nuestra mejor corrida ha sido con una arquitectura de [2 9 9 1],
un learning rate de 0.4, un momentum de 1 utilizando la función sigmoidea lógica
y los siguientes parámetros:

* Tres pasos son considerados una buena racha.
* A eta se le suma a 0.01 en una buena racha.
* Eta se disminuye por un factor 0.01 en un paso considerado malo.
* Se evalúa si el paso es bueno, malo o neutral según un epsilon de 0.01.

Con esta configuración se obtiene un error para el conjunto completo de 0.13238.

